# Airflow Configuration for Sentiment Analysis Pipeline

[core]
# The executor to use (SequentialExecutor, LocalExecutor, CeleryExecutor, etc.)
executor = LocalExecutor

# The folder where your airflow pipelines live
dags_folder = /home/felipe/sentiment-project/airflow/dags

# The base log folder
base_log_folder = /home/felipe/sentiment-project/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search
remote_logging = False

# Logging level
logging_level = INFO

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 1

# Whether to load the examples that ship with Airflow
load_examples = False

[database]
# The SQLAlchemy connection string to the metadata database
sql_alchemy_conn = sqlite:////home/felipe/sentiment-project/airflow/airflow.db

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using
base_url = http://localhost:8080

# The port on which to run the web server
web_server_port = 8080

# Secret key used to run your flask app
secret_key = temporary_key_change_in_production

# Number of workers to run the Gunicorn web server
workers = 4

[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# The number of times to try to schedule each DAG file
num_runs = 5

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via `default_args`
default_owner = airflow

[mlflow]
# MLflow tracking URI
tracking_uri = http://localhost:5000

# Default experiment name
default_experiment = sentiment-analysis-pipeline
